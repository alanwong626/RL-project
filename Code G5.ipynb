{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exist, back up to back_up_dino_log.csv\n",
      "directory= C:\\Users\\dswon\\Desktop\n",
      "iteration: 0 ,score: 4 ,hi_score: 0 ,elapsed time: 8.707766771316528 ,epsilon: 0.1 ,action: 1 ,reward: 0.0 ,Q max: 0.0027783886\n",
      "iteration=  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dswon\\anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type DQN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "games=  5\n",
      "games=  10\n",
      "games=  15\n",
      "games=  20\n",
      "games=  25\n",
      "games=  30\n",
      "games=  35\n",
      "games=  40\n",
      "games=  45\n",
      "games=  50\n",
      "games=  55\n",
      "games=  60\n",
      "games=  65\n",
      "games=  70\n",
      "games=  75\n",
      "games=  80\n",
      "games=  85\n",
      "games=  90\n",
      "games=  95\n",
      "games=  100\n",
      "games=  105\n",
      "games=  110\n",
      "games=  115\n",
      "games=  120\n",
      "games=  125\n",
      "games=  130\n",
      "games=  135\n",
      "games=  140\n",
      "games=  145\n",
      "games=  150\n",
      "games=  155\n",
      "games=  160\n",
      "games=  165\n",
      "games=  170\n",
      "games=  175\n",
      "games=  180\n",
      "games=  185\n",
      "games=  190\n",
      "games=  195\n",
      "games=  200\n",
      "games=  205\n",
      "games=  210\n",
      "games=  215\n",
      "games=  220\n",
      "games=  225\n",
      "games=  230\n",
      "games=  235\n",
      "games=  240\n",
      "games=  245\n",
      "games=  250\n",
      "games=  255\n",
      "games=  260\n",
      "games=  265\n",
      "games=  270\n",
      "games=  275\n",
      "games=  280\n",
      "games=  285\n",
      "games=  290\n",
      "games=  295\n",
      "games=  300\n",
      "games=  305\n",
      "games=  310\n",
      "games=  315\n",
      "games=  320\n",
      "games=  325\n",
      "games=  330\n",
      "games=  335\n",
      "games=  340\n",
      "games=  345\n",
      "games=  350\n",
      "games=  355\n",
      "games=  360\n",
      "iteration: 50000 ,score: 125 ,hi_score: 495 ,elapsed time: 3474.4812684059143 ,epsilon: 0.05004999999992368 ,action: 0 ,reward: 0.1 ,Q max: 3.329702\n",
      "iteration=  50000\n",
      "games=  365\n",
      "games=  370\n",
      "games=  375\n",
      "games=  380\n",
      "games=  385\n",
      "games=  390\n",
      "games=  395\n",
      "games=  400\n",
      "games=  405\n",
      "games=  410\n",
      "games=  415\n",
      "games=  420\n",
      "games=  425\n",
      "games=  430\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import gym_chrome_dino\n",
    "from gym_chrome_dino.utils.wrappers import make_dino\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "import os , sys\n",
    "import time\n",
    "\n",
    "#from PIL import Image\n",
    "#import cv2 #opencv\n",
    "#import io\n",
    "#from io import BytesIO\n",
    "#from IPython.display import clear_output\n",
    "#import json\n",
    "#import base64\n",
    "\n",
    "\n",
    "#########################################################################################################################df = pd.DataFrame(columns = [\"Data_Time\",\"Iteration\", \"Average_Score\", \"High_Score\", \"Epsilon\"])\n",
    "column_names = [\"Mintues\",\"Seconds\",\"Games\",\"iteration\",\"Average_Score\", \"High_Score\", \"Epsilon\"]\n",
    "df = pd.DataFrame(columns = column_names)\n",
    "df.columns = [col.replace(',', '') for col in df.columns]\n",
    "# Back up the pervious csv file incase ran by accident\n",
    "if Path(\"./dino_log.csv\").is_file():\n",
    "    temp = pd.read_csv(\"./dino_log.csv\")\n",
    "    temp.to_csv(\"back_up_\" + \"dino_log.csv\")\n",
    "    print(\"File exist, back up to back_up_dino_log.csv\")\n",
    "\n",
    "df.to_csv(\"./dino_log.csv\")\n",
    "\n",
    "# formatter = logging.Formatter(r'\"%(asctime)s\",%(message)s')\n",
    "formatter = logging.Formatter(r'%(message)s')\n",
    "logger = logging.getLogger(\"dino_rl\")\n",
    "logger.setLevel(logging.INFO)\n",
    "filehandler_dbg = logging.FileHandler(logger.name + '-debug.log', mode='w')\n",
    "fh = logging.FileHandler(\"./dino_log.csv\")\n",
    "\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "#########################################################################################################################\n",
    "#########################################################################################################################\n",
    "#os.chdir(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "print (\"directory=\",os.getcwd())\n",
    "#########################################################################################################################\n",
    "class DQN(nn.Module):  #DQN\n",
    "\n",
    "#     def __init__(self, num_actions):\n",
    "#         super(DQN, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(4, 32, 8, 4)\n",
    "#         self.relu1 = nn.ReLU(inplace=True)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "#         self.relu2 = nn.ReLU(inplace=True)\n",
    "#         self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "#         self.relu3 = nn.ReLU(inplace=True)\n",
    "#         self.fc4 = nn.Linear(3136, 512) #fully connected\n",
    "#         self.relu4 = nn.ReLU(inplace=True)\n",
    "#         self.fc5 = nn.Linear(512 , num_actions)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         #print(\"forward_ok\")\n",
    "#         y = self.conv1(x)\n",
    "#         y = self.relu1(y)\n",
    "#         y = self.conv2(y)\n",
    "#         y = self.relu2(y)\n",
    "#         y = self.conv3(y)\n",
    "#         y = self.relu3(y)\n",
    "#         y = y.view(y.size()[0], -1)\n",
    "#         y = self.fc4(y)\n",
    "#         y = self.relu4(y)\n",
    "#         y = self.fc5(y)\n",
    "\n",
    "#         return y\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.number_of_actions = num_actions\n",
    "#         self.gamma = 0.99\n",
    "#         self.final_epsilon = 0.0001\n",
    "#         self.initial_epsilon = 0.1\n",
    "#         self.number_of_iterations = 5000000\n",
    "#         self.replay_memory_size = 10000\n",
    "#         self.minibatch_size = 32\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.fc4 = nn.Linear(3136, 512)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.fc5 = nn.Linear(512 , self.number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.relu3(y)\n",
    "        y = y.view(y.size()[0], -1)\n",
    "        y = self.fc4(y)\n",
    "        y = self.relu4(y)\n",
    "        y = self.fc5(y)\n",
    "\n",
    "        return y\n",
    "#########################################################################################################################\n",
    "class DQN_BN(nn.Module):  # DQN with batch normalization\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN_BN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.fc4 = nn.Linear(3136, 512) #fully connected\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.fc5 = nn.Linear(512 , num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.bn2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.bn3(y)\n",
    "        y = self.relu3(y)\n",
    "        y = y.view(y.size()[0], -1)\n",
    "        y = self.fc4(y)\n",
    "        y = self.relu4(y)\n",
    "        y = self.fc5(y)\n",
    "\n",
    "        return y\n",
    "########################################################################################################################\n",
    "class DQN_duel(nn.Module):  #DQN + dueling\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        super(DQN_duel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.fc4_a = nn.Linear(3136, 512)\n",
    "        self.relu4_a = nn.ReLU(inplace=True)\n",
    "        self.fc4_v = nn.Linear(3136, 512)\n",
    "        self.relu4_v = nn.ReLU(inplace=True)\n",
    "        self.fc5_a = nn.Linear(512, num_actions)\n",
    "        self.fc5_v = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.conv3(y)\n",
    "        y = self.relu3(y)\n",
    "        y = y.view(y.size()[0], -1)\n",
    "        y_a=self.fc4_a(y)\n",
    "        y_a=self.relu4_a(y_a)\n",
    "        y_v=self.fc4_v(y)\n",
    "        y_v=self.relu4_v(y_v)\n",
    "        y_a=self.fc5_a(y_a)\n",
    "        y_v=self.fc5_v(y_v).expand_as(y_a)\n",
    "        y= y_v + y_a - y_a.mean(1).unsqueeze(1).expand_as(y_a)\n",
    "\n",
    "        return y\n",
    "########################################################################################################################\n",
    "def train(net, target_net, start, device, logger, start_iter=0):\n",
    "    global alpha, gamma, epsilon_i, batch_size, replay_mem_size, steps, num_actions, epoch_size, update_size\n",
    "    start_time = time.time()\n",
    "    hi_score = 0\n",
    "    games=0\n",
    "    iteration = start_iter\n",
    "    replay_memory = []\n",
    "    epsilon = epsilon_i\n",
    "    optimizer=optim.RMSprop(net.parameters(), alpha)\n",
    "    #########################################################\n",
    "#     env = gym.make('ChromeDino-v0')\n",
    "    env = gym.make('ChromeDinoNoBrowser-v0') #\n",
    "    env = make_dino(env, timer=True, frame_stack=True)\n",
    "    image_0 = env.reset()\n",
    "    ##################################################################################\n",
    "    state = convert(image_0).to(device)\n",
    "    ################## main loop #################################################################\n",
    "    generation_score = []\n",
    "    while iteration < steps:\n",
    "        #print(\"forward_call\")\n",
    "        output = net(state)[0]\n",
    "        action = torch.zeros([num_actions], dtype=torch.float32)#.to(device)\n",
    "        random_action = random.random() <= epsilon\n",
    "        action_index = [ torch.randint( num_actions, torch.Size([]), dtype=torch.int8 )#.to(device)\n",
    "                        if random_action\n",
    "                        else torch.argmax(output)][0]\n",
    "        action[action_index] = 1\n",
    "        if action[0] == 1:\n",
    "            act=0 #do nothing\n",
    "        elif action[1] == 1:\n",
    "            act=1 #jump\n",
    "        ############################################################################\n",
    "        image_1, reward, terminal, info = env.step(act)\n",
    "        state_1= convert(image_1).to(device)\n",
    "        action = action.unsqueeze(0)\n",
    "        reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0).to(device)\n",
    "        ############ REPLAY MEMORY ############################################################\n",
    "        replay_memory.append((state, action, reward, state_1, terminal))\n",
    "        if len(replay_memory) > replay_mem_size:\n",
    "            replay_memory.pop(0)\n",
    "        batch = random.sample(replay_memory, min( len(replay_memory), batch_size ) )\n",
    "        ############################################################################################\n",
    "        state_batch = torch.cat(tuple(t[0] for t in batch)).to(device)\n",
    "        action_batch = torch.cat(tuple(t[1] for t in batch)).to(device)\n",
    "        reward_batch = torch.cat(tuple(t[2] for t in batch)).to(device)\n",
    "        state_1_batch = torch.cat(tuple(t[3] for t in batch)).to(device)\n",
    "        #print(\"forward_call\")\n",
    "        output_1_batch = target_net(state_1_batch)#.to(device)\n",
    "        y_batch = torch.cat(tuple(reward_batch[i] if batch[i][4]\n",
    "                                  else reward_batch[i] + gamma * torch.max(output_1_batch[i])\n",
    "                                  for i in range(len(batch))))#.to(device)\n",
    "        #print(\"forward_call\")\n",
    "        q_value = torch.sum( net( state_batch ) * action_batch, dim=1 ).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_batch = y_batch.detach()\n",
    "        loss = F.smooth_l1_loss(q_value, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        state = state_1\n",
    "        ######  print output routine and reset env ###################################################\n",
    "#         generation_score = []\n",
    "        score = env.unwrapped.game.get_score()\n",
    "        #print(\"terminal=\",terminal)\n",
    "        if terminal == True:\n",
    "            games +=1\n",
    "            if games % update_size == 0:\n",
    "                target_net.load_state_dict(net.state_dict())\n",
    "                print(\"games= \",games)\n",
    "            generation_score.append(score)\n",
    "            time.sleep(0.1)\n",
    "            env.reset()\n",
    "            if score > hi_score:\n",
    "                hi_score=score\n",
    "            if games % epoch_size == 0:\n",
    "                if games == 0:\n",
    "                    average_score = 0\n",
    "                    duration = start_time\n",
    "                else:\n",
    "                    average_score = sum(generation_score)/len(generation_score)\n",
    "                    duration = time.time() - start_time\n",
    "                dino_log(duration, games, iteration,average_score, hi_score, epsilon, logger)\n",
    "                generation_score = []\n",
    "#         if games%5 == 0:\n",
    "#             if games == 0:\n",
    "#                 average_score = 0\n",
    "#                 dino_log(games, average_score, hi_score, epsilon, logger)\n",
    "\n",
    "#             elif len(generation_score) == 0:\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 average_score = sum(generation_score)/len(generation_score)\n",
    "#                 dino_log(games, average_score, hi_score, epsilon, logger)\n",
    "# #             generation_score = []\n",
    "        if iteration % 50000 == 0:\n",
    "            print(\"iteration:\", iteration, \",score:\", score, \",hi_score:\", hi_score,\n",
    "                  \",elapsed time:\", time.time() - start, \",epsilon:\", epsilon, \",action:\",\n",
    "                  #action_index.cpu().detach().numpy(), \",reward:\", reward.numpy()[0][0], \",Q max:\",\n",
    "                  action_index.cpu().detach().numpy(), \",reward:\", reward.cpu().numpy()[0][0], \",Q max:\",\n",
    "                  np.max(output.cpu().detach().numpy()))\n",
    "#             print(\"iteration:\", iteration, \",score:\", score, \",hi_score:\", hi_score,\n",
    "#                   \",elapsed time:\", time.time() - start, \",epsilon:\", epsilon, \",action:\",\n",
    "#                   #action_index.cpu().detach().numpy(), \",reward:\", reward.numpy()[0][0], \",Q max:\",\n",
    "#                   action_index.cpu().detach().numpy(), \",reward:\", reward.cpu().numpy()[0][0], \",Q max:\",\n",
    "#                   np.max(output.cpu().detach().numpy()), \",avg_score:\", average_score)\n",
    "\n",
    "\n",
    "        if iteration % 50000 == 0:\n",
    "            torch.save(net, \"trained-model/current_model_\" + str(iteration) + \".pth\")\n",
    "        if iteration % 50000 == 0:\n",
    "            print(\"iteration= \",iteration)\n",
    "        #############################################################################\n",
    "        epsilon = linDecay( epsilon )\n",
    "        iteration += 1\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "def dino_log(duration, games, iteration, average_score, hi_score,epsilon,logger):\n",
    "#     duration = (int(duration//60)+round((duration%60)/100,2))\n",
    "    mintue = int(duration//60)\n",
    "    seconds = int(duration%60)\n",
    "    space = str(\" \")\n",
    "#     print(str(mintue),seconds)\n",
    "    logger.info(f\"{space},{mintue},{seconds},{games},{iteration},{average_score},{hi_score},{epsilon}\")\n",
    "#####################################################################################\n",
    "def test(model, device):\n",
    "\n",
    "    env = gym.make('ChromeDino-v0')\n",
    "    env = make_dino(env, timer=True, frame_stack=True)\n",
    "    image_0 = env.reset()\n",
    "    state = convert(image_0).to(device)\n",
    "    while True:\n",
    "        output = model(state)[0]\n",
    "\n",
    "        action = torch.zeros([model.number_of_actions], dtype=torch.int8)#.to(device)\n",
    "        action_index = torch.argmax(output)#.to(device)\n",
    "        action[action_index] = 1\n",
    "\n",
    "        if action[0] == 1:\n",
    "            act=0 #do nothing\n",
    "        elif action[1] == 1:\n",
    "            act=1 #jump\n",
    "\n",
    "        image_1, reward, terminal, info = env.step(act)\n",
    "        state_1= convert(image_1).to(device)\n",
    "\n",
    "        state = state_1\n",
    "#####################################################################################\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        torch.nn.init.uniform_(m.weight, -0.01, 0.01)#.to(device)\n",
    "        m.bias.data.fill_(0.01)\n",
    "####################################################################################################\n",
    "def linDecay(eps):\n",
    "    global epsilon_i, epsilon_f, epsilon_decay_steps\n",
    "    start_eps = epsilon_i\n",
    "    end_eps = epsilon_f\n",
    "    tot_steps = epsilon_decay_steps\n",
    "    decay_rate = (start_eps - end_eps) / tot_steps\n",
    "    if eps > end_eps:\n",
    "        eps -= decay_rate\n",
    "    return eps\n",
    "####################################################################################################\n",
    "def convert(image):\n",
    "    state = np.transpose(image, (2, 0, 1))\n",
    "    state = np.ascontiguousarray(state, dtype=np.float32) / 255\n",
    "    state = torch.from_numpy(state)\n",
    "    return state.unsqueeze(0)\n",
    "#####################################################################################\n",
    "## MAIN #########\n",
    "\n",
    "### input parameters ##############\n",
    "#global alpha, gamma, epsilon_i, epsilon_f, epsilon_decay_steps, batch_size, replay_mem_size, steps, num_actions\n",
    "alpha = 2e-5\n",
    "gamma = 0.99\n",
    "epsilon_i = 0.1\n",
    "epsilon_f = 0.0001\n",
    "epsilon_decay_steps =  100000\n",
    "batch_size = 128\n",
    "replay_mem_size = 30000\n",
    "steps = 2500000\n",
    "epoch_size = 1\n",
    "num_actions = 2 # 0=do-nothing, 1=jump\n",
    "update_size = 5\n",
    "###################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##comment accordingly if test, train or restart-train mode###\n",
    "mode=\"train\"\n",
    "#mode=\"test\"\n",
    "#mode=\"restart\"\n",
    "\n",
    "if mode==\"train\":\n",
    "    if not os.path.exists('trained-model/'):\n",
    "        os.mkdir('trained-model/')\n",
    "    net = DQN( num_actions ).to(device)\n",
    "    #net = DQN_BN( num_actions ).to(device)\n",
    "#     net = DQN_duel( num_actions ).to(device)\n",
    "    target_net = DQN(num_actions).to(device)\n",
    "    #target_net = DQN_BN(num_actions).to(device)\n",
    "#     target_net = DQN_duel(num_actions).to(device)\n",
    "#     net.apply(init_weights)\n",
    "    target_net.load_state_dict(net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    start = time.time()\n",
    "    train(net, target_net, start, device, logger).to(device)\n",
    "elif mode==\"restart\":\n",
    "    iterations=250000 #insert the number of iterations of previous run\n",
    "    net = torch.load('trained-model/restart/restart_model.pth', map_location='cpu').eval()\n",
    "    net = net.to(device)\n",
    "    start = time.time()\n",
    "    train(net, start, device, logger, iterations).to(device)\n",
    "elif mode==\"test\":\n",
    "    net = torch.load('trained-model/trained_agents/current_model_250000.pth', map_location='cpu').eval()\n",
    "    net = net.to(device)\n",
    "    test(net, device)#.to(device)\n",
    "####################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
